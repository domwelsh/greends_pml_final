{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, accuracy_score, make_scorer\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import scipy.stats as stats\n",
    "from torchsummary import summary\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.base import BaseEstimator\n",
    "from torchmetrics import MeanSquaredError, R2Score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./leap-atmospheric-physics-ai-climsim/train.csv', sep=',', nrows=2000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_df = df.iloc[:, 1:557]\n",
    "output_df = df.iloc[:, 557:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n",
      "[ 60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77\n",
      " 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137\n",
      " 138 139 140 141 142 143 144 145 146 147 148 180 181 182 183 184 185 186\n",
      " 187 188 189 190 191 192 193 194 195 196 376 377 378 379 380 381 382 388\n",
      " 389 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410\n",
      " 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428\n",
      " 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446\n",
      " 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464\n",
      " 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482\n",
      " 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500\n",
      " 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518\n",
      " 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536\n",
      " 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554\n",
      " 555]\n"
     ]
    }
   ],
   "source": [
    "# check which columns have very small variance\n",
    "low_variance_cols = np.where(np.std(input_df, axis=0) < 1e-6)[0]\n",
    "print(len(low_variance_cols))\n",
    "print(low_variance_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/function_base.py:2897: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[:, None]\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/numpy/lib/function_base.py:2898: RuntimeWarning: invalid value encountered in divide\n",
      "  c /= stddev[None, :]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 64, 65, 66, 70, 71, 72, 73, 74, 75, 76, 77, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 183, 184, 188, 189, 190, 191, 192, 193, 194, 195, 196, 376, 377, 378, 379, 380, 381, 382, 388, 389, 394, 395, 396, 397, 398, 399, 400, 401, 402, 403, 404, 405, 406, 407, 408, 409, 410, 411, 412, 413, 414, 415, 416, 417, 418, 419, 420, 421, 422, 423, 424, 425, 426, 427, 428, 429, 430, 431, 432, 433, 434, 435, 436, 437, 438, 439, 440, 441, 442, 443, 444, 445, 446, 447, 448, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 496, 497, 498, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 511, 512, 513, 514, 515, 516, 517, 518, 519, 520, 521, 522]\n",
      "153\n"
     ]
    }
   ],
   "source": [
    "# check if any of small variance features have correlation with outputs\n",
    "correlated_cols = []\n",
    "\n",
    "for col_index in low_variance_cols:\n",
    "    correlation_check = 0\n",
    "    for output_col in output_df.columns:\n",
    "        input_col = input_df.columns[col_index]\n",
    "        corr_coef = input_df[input_col].corr(output_df[output_col])\n",
    "        if -0.1 <= corr_coef <= 0.1 or np.isnan(corr_coef):\n",
    "            continue\n",
    "        else:\n",
    "            correlation_check = 1\n",
    "            break\n",
    "    if correlation_check == 1:\n",
    "        correlated_cols.append(col_index)\n",
    "\n",
    "print(correlated_cols)\n",
    "print(len(correlated_cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n",
      "[523, 524, 525, 526, 527, 528, 529, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 554, 555, 60, 61, 62, 67, 68, 69, 120, 121, 122, 123, 180, 181, 182, 185, 186, 187, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484, 485, 486, 487, 488, 489, 490, 491, 492, 493, 494, 495]\n"
     ]
    }
   ],
   "source": [
    "# create list of features with no correlation\n",
    "set1 = set(low_variance_cols)\n",
    "set2 = set(correlated_cols)\n",
    "\n",
    "no_corr_or_var = list(set1 - set2)\n",
    "print(len(no_corr_or_var))\n",
    "print(no_corr_or_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2000000 entries, 0 to 1999999\n",
      "Columns: 474 entries, state_t_0 to pbuf_N2O_26\n",
      "dtypes: float64(474)\n",
      "memory usage: 7.1 GB\n"
     ]
    }
   ],
   "source": [
    "# drop low variance with no correlation columns\n",
    "input_df_clean = input_df.drop(input_df.columns[no_corr_or_var], axis=1)\n",
    "input_df_clean.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of input features not normally distributed = 473 out of 474\n"
     ]
    }
   ],
   "source": [
    "# check normal distribution, use MinMaxScaler if most features are not normally distributed\n",
    "p_threshold = 0.05\n",
    "not_norm_num = 0\n",
    "for col in input_df_clean.iloc[:, 1:].columns:\n",
    "    stat, p = stats.normaltest(df[col])\n",
    "    if p <= p_threshold:\n",
    "        not_norm_num += 1\n",
    "\n",
    "print(f'Number of input features not normally distributed = {not_norm_num} out of {len(input_df_clean.columns)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
